{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\ML_Projects\\\\Predictive_Maintenance\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\ML_Projects\\\\Predictive_Maintenance'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.exception import CustomException\n",
    "from src.logger import logging\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    preprocessor_obj_file_path = os.path.join(\"artifacts\", \"preprocessor.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(file_path, obj):\n",
    "    \n",
    "    try:\n",
    "        dir_path = os.path.dirname(file_path)\n",
    "        \n",
    "        os.makedirs(dir_path, exist_ok = True)\n",
    "        \n",
    "        with open(file_path, \"wb\") as file_obj:\n",
    "            pickle.dump(obj, file_obj)\n",
    "                   \n",
    "    except Exception as e:\n",
    "        raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self):\n",
    "        self.data_transformation_config = DataTransformationConfig()\n",
    "        \n",
    "    def get_data_trasnformer_object(self):\n",
    "        try:\n",
    "            num_cols = ['Engine_no', 'Cycle_no',\n",
    "            'LPC_outlet_temperature',\n",
    "            'HPC_outlet_temperature', 'LPT_outlet_temperature',\n",
    "            'HPC_outlet_pressure',\n",
    "            'Physical_fan_speed', 'Physical_core_speed', \n",
    "            'HPC_outlet_static_pressure', 'Fuel_flow_ratio', 'Fan_speed',\n",
    "            'Bypass_ratio', 'Bleed_enthalpy',\n",
    "            'High_pressure_cool_air_flow', 'Low_pressure_cool_air_flow']\n",
    "            \n",
    "            num_pipeline = Pipeline(\n",
    "                \n",
    "                steps = [\n",
    "                    (\"scaler\", RobustScaler()),\n",
    "                    (\"PCA\", PCA(n_components=8))\n",
    "                    \n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            logging.info(\"Pipeline created\")\n",
    "            \n",
    "            preprocessor = ColumnTransformer(\n",
    "                \n",
    "                [\n",
    "                    (\"num_pipeline\", num_pipeline, num_cols)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            return preprocessor\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "        \n",
    "    def initiate_data_transformation(self, train_path, test_path):\n",
    "        \n",
    "        try:\n",
    "            train_df = pd.read_csv(train_path)\n",
    "            test_df = pd.read_csv(test_path)\n",
    "            \n",
    "            train_df[\"RUL\"][train_df[\"RUL\"] > 103] = 103\n",
    "            test_df[\"RUL\"][test_df[\"RUL\"] > 103] = 103\n",
    "            \n",
    "            logging.info(\"Train & test data readed\")\n",
    "            \n",
    "            target_column_name = \"RUL\"\n",
    "            drop_cols = [\n",
    "                'Setting_1', 'Setting_2', 'Setting_3', 'Fan_inlet_temperature', 'Fan_inlet_pressure', \n",
    "                'Bypass_duct_pressure', 'Engine_pressure_ratio', 'Core_speed', 'Burner_fuel_air_ratio', \n",
    "                'Required_fan_speed', 'Required_fan_conversion_speed']  \n",
    "            \n",
    "            \n",
    "            input_feature_train_df = train_df.drop(columns=[target_column_name] + drop_cols)\n",
    "            target_feature_train_df = train_df[target_column_name]\n",
    "            \n",
    "            input_feature_test_df = test_df.drop(columns=[target_column_name] + drop_cols)\n",
    "            target_feature_test_df = test_df[target_column_name]\n",
    "            \n",
    "            \n",
    "            preprocessor_obj = self.get_data_trasnformer_object()\n",
    "            logging.info(\"Applying preprocessing obj on train & test dataframe\")\n",
    "            \n",
    "        #Transforming into Preprocessor object to data\n",
    "            input_feature_train_arr = preprocessor_obj.fit_transform(input_feature_train_df) #train data\n",
    "            input_feature_test_arr = preprocessor_obj.transform(input_feature_test_df) #test data\n",
    "            logging.info(\"Preprocessing applied to training & test datasets\") \n",
    "            \n",
    "            #Converting into numpy array for train & test data\n",
    "            train_arr = np.c_[input_feature_train_arr, np.array(target_feature_train_df)] #train array \n",
    "            test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)] #test array\n",
    "            logging.info(\"Data transfom into the array\")\n",
    "            \n",
    "            #function from utils to save the preprocessor file\n",
    "            save_object(\n",
    "                file_path=self.data_transformation_config.preprocessor_obj_file_path,\n",
    "                obj = preprocessor_obj\n",
    "            )\n",
    "            logging.info(\"Preprocessor file saved as Pickle file\")\n",
    "            \n",
    "            return (\n",
    "                train_arr,\n",
    "                test_arr\n",
    "            )\n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.components.data_ingestion import DataIngestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ML_Projects\\Predictive_Maintenance\\src\\utils.py:33: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_train = pd.read_sql_query(\"select * from train1\", mydb)\n",
      "e:\\ML_Projects\\Predictive_Maintenance\\src\\utils.py:35: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_test = pd.read_sql_query(\"select * from test1\", mydb)\n",
      "C:\\Users\\Ayush Gandhi\\AppData\\Local\\Temp\\ipykernel_13324\\3415109918.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[\"RUL\"][train_df[\"RUL\"] > 103] = 103\n",
      "C:\\Users\\Ayush Gandhi\\AppData\\Local\\Temp\\ipykernel_13324\\3415109918.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[\"RUL\"][test_df[\"RUL\"] > 103] = 103\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    obj = DataIngestion()\n",
    "    train_data, test_data = obj.initiate_data_ingestion()\n",
    "    \n",
    "    data_transformation = DataTransformation()\n",
    "    data_transformation.initiate_data_transformation(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
